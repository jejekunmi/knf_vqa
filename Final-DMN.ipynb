{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules needed\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.ion()\n",
    "\n",
    "import util\n",
    "from vqa_model import Encoder, Decoder, VQASystem, AttnGRU, AttnGRUCell\n",
    "from squeezenet import SqueezeNet\n",
    "from vgg16 import VGG16\n",
    "\n",
    "from compact_bilinear_pooling import compact_bilinear_pooling_layer\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data (excluding images)\n",
    "\n",
    "# Restrict the number of possible answers using this\n",
    "# Decreasing this will increase the number of classes \n",
    "train_min_count = 99\n",
    "val_cutoff = 107183\n",
    "# Load data\n",
    "dataset = util.load_data_all(train_min_count, val_cutoff=107183, limit=100000000)\n",
    "np_embeddings = np.load(\"data/glove.trimmed.100.npz\")[\"glove\"]\n",
    "answer_to_id, id_to_answer = util.load_answer_map(min_count=train_min_count)\n",
    "with open('data/qid_to_anstype.dat', 'rb') as fp:\n",
    "    qid_to_anstype = pickle.load(fp)\n",
    "\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_train\", dataset.train.questions.shape)\n",
    "print(\"Questions_mask_train\", dataset.train.mask.shape)\n",
    "print(\"Questions_ids_train\", dataset.train.question_ids.shape)\n",
    "print(\"Image_ids_train\", dataset.train.image_ids.shape)\n",
    "print(\"All_answers_train\", dataset.train.all_answers.shape)\n",
    "print(\"Answers_train\", dataset.train.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_val\", dataset.val.questions.shape)\n",
    "print(\"Questions_mask_val\", dataset.val.mask.shape)\n",
    "print(\"Questions_ids_val\", dataset.val.question_ids.shape)\n",
    "print(\"Image_ids_val\", dataset.val.image_ids.shape)\n",
    "print(\"All_answers_val\", dataset.val.all_answers.shape)\n",
    "print(\"Answers_val\", dataset.val.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_test\", dataset.test.questions.shape)\n",
    "print(\"Questions_mask_test\", dataset.test.mask.shape)\n",
    "print(\"Questions_ids_test\", dataset.test.question_ids.shape)\n",
    "print(\"Image_ids_test\", dataset.test.image_ids.shape)\n",
    "print(\"All_answers_test\", dataset.test.all_answers.shape)\n",
    "print(\"Answers_test\", dataset.test.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"np_embeddings\", np_embeddings.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"There are\", len(answer_to_id), \"possible answers (including <unk>)\")\n",
    "print(\"This should be less than or equal to above\", np.max(dataset.train.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.val.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.test.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.train.all_answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.val.all_answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.test.all_answers) + 1) \n",
    "print(\"*\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "    \"\"\"\n",
    "    epochs = 10\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    lr_decay = 0.9\n",
    "    optimizer = tf.train.AdamOptimizer\n",
    "    max_gradient_norm = 10.0\n",
    "    clip_gradients = True\n",
    "    dropout_keep_prob = 1.0\n",
    "    l2_reg = 0.0\n",
    "    weight_decay = 5e-4\n",
    "    batch_size = 32\n",
    "    train_all = True\n",
    "    \n",
    "    train_limit = 10000000000\n",
    "    \n",
    "    max_question_length = 25    \n",
    "    num_answers_per_question = 10\n",
    "    num_classes = len(answer_to_id)\n",
    "    image_size = [224, 224, 3]\n",
    "    cbpl_output_dim = 8000\n",
    "    att_conv1_dim = 512\n",
    "    num_levels = 2\n",
    "    \n",
    "    batch_normalize = False\n",
    "    \n",
    "    vgg_out_dim = [14, 14]\n",
    "    \n",
    "    images_input_sequece_len = vgg_out_dim[0] * vgg_out_dim[1]\n",
    "    \n",
    "    rnn_hidden_size = 200 # RNN\n",
    "    fc_state_size = 100 # Fully connected\n",
    "    embedding_size = 100\n",
    "    image_seq_size = 200\n",
    "    fact_size = rnn_hidden_size\n",
    "    \n",
    "    \n",
    "    num_evaluate = 5000\n",
    "    \n",
    "    eval_every = 50\n",
    "    print_every = 100 \n",
    "    \n",
    "    VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "    \n",
    "    model_dir = \"final_dmn\"\n",
    "    squeeze_net_dir = \"sq_net_model/squeezenet.ckpt\"\n",
    "    vgg16_weight_file = \"vgg_net_dir/vgg_16.ckpt\"\n",
    "    vgg_out_dim = [14, 14]\n",
    "    \n",
    "    vgg_exclude_names = ['qa/vgg_16/pool5', 'qa/vgg_16/fc6', 'qa/vgg_16/fc7', 'qa/vgg_16/fc8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaselineEncoder(Encoder):\n",
    "    def get_image_seq(self, images):\n",
    "        with tf.variable_scope('image_facts'):\n",
    "            image_seq = tf.reshape(images, \n",
    "                                   shape=(-1, self.config.images_input_sequece_len, 512))\n",
    "            print(\"init_image_seq\", image_seq)\n",
    "            \n",
    "            W_facts = tf.get_variable(name=\"W_facts\", \n",
    "                                     shape=(512, self.config.image_seq_size), \n",
    "                                     dtype=tf.float32, \n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_facts = tf.get_variable(name=\"b_facts\", \n",
    "                                    shape=(self.config.image_seq_size,), \n",
    "                                    dtype=tf.float32, \n",
    "                                    initializer=tf.constant_initializer(0.0))\n",
    "            seq_reshaped = tf.reshape(image_seq,\n",
    "                                     shape=[-1, 512])\n",
    "            print(\"seq_reshaped\", seq_reshaped)\n",
    "            seq_linear = tf.tanh(tf.matmul(seq_reshaped, W_facts) + b_facts)\n",
    "            image_seq = tf.reshape(seq_linear, \n",
    "                           shape=(-1, self.config.images_input_sequece_len, self.config.image_seq_size))\n",
    "            print(\"image_seq\", image_seq)\n",
    "            \n",
    "            return image_seq\n",
    "    \n",
    "    def get_attenton_g(self, f, q, m_prev):\n",
    "        with tf.variable_scope('attention_g'):\n",
    "            W_att_1 = tf.get_variable(name=\"W_att_1_weight\", \n",
    "                                      shape=(4*self.config.rnn_hidden_size, \n",
    "                                             self.config.rnn_hidden_size), \n",
    "                                      dtype=tf.float32, \n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W_att_2 = tf.get_variable(name=\"W_att_2_weight\", \n",
    "                                             shape=(self.config.rnn_hidden_size, 1), \n",
    "                                             dtype=tf.float32, \n",
    "                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_att_1 = tf.get_variable(name=\"b_att_1\", \n",
    "                                            shape=(self.config.rnn_hidden_size,), \n",
    "                                            dtype=tf.float32, \n",
    "                                            initializer=tf.constant_initializer(0.0))\n",
    "            b_att_2 = tf.get_variable(name=\"b_att_2\", \n",
    "                                            shape=(), \n",
    "                                            dtype=tf.float32, \n",
    "                                            initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "            q = tf.expand_dims(q, axis=1)\n",
    "            m_prev = tf.expand_dims(m_prev, axis=1)\n",
    "            print(\"q\", q)\n",
    "            print(\"m_prev\", m_prev)\n",
    "            z = tf.concat((f * q, f * m_prev, \n",
    "                           tf.abs(f - q), tf.abs(f - m_prev)), \n",
    "                          axis=2)\n",
    "            print(\"z\", z)\n",
    "            \n",
    "            z_reshaped = tf.reshape(z, [-1, 4*self.config.rnn_hidden_size])\n",
    "            print(\"z_reshaped\", z_reshaped)\n",
    "            Z_temp = tf.tanh(tf.matmul(z_reshaped, W_att_1) + b_att_1)\n",
    "            print(\"Z_temp\", Z_temp)\n",
    "            \n",
    "            Z_reshaped = tf.matmul(Z_temp, W_att_2) + b_att_2\n",
    "            print(\"Z_reshaped\", Z_reshaped)\n",
    "            \n",
    "            Z = tf.reshape(Z_reshaped, [-1, self.config.images_input_sequece_len])\n",
    "            print(\"Z\", Z)\n",
    "            \n",
    "            exp_Z = tf.exp(Z)\n",
    "            g = exp_Z / (1e-6 + tf.reduce_sum(exp_Z, reduction_indices=1, keep_dims=True))\n",
    "            print(\"g\", g)\n",
    "                        \n",
    "            return g\n",
    "    \n",
    "    def get_next_memory_state(self, m_prev, c, q):\n",
    "        with tf.variable_scope('compute_next_m'):\n",
    "            combo = tf.concat((m_prev, c, q), axis=1)\n",
    "            print(\"combo\",combo)\n",
    "            \n",
    "            W_nm_1 = tf.get_variable(name=\"W_nm_1_weight\", \n",
    "                                      shape=(3*self.config.rnn_hidden_size, \n",
    "                                             self.config.rnn_hidden_size), \n",
    "                                      dtype=tf.float32, \n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_nm_1 = tf.get_variable(name=\"b_nm_1\", \n",
    "                                            shape=(self.config.rnn_hidden_size,), \n",
    "                                            dtype=tf.float32, \n",
    "                                            initializer=tf.constant_initializer(0.0))\n",
    "            m = tf.matmul(combo, W_nm_1) \n",
    "            if self.config.batch_normalize:\n",
    "                m = tf.layers.batch_normalization(m, training=self.is_training)\n",
    "            m = tf.nn.relu(m + b_nm_1) \n",
    "            print(\"m\", m)\n",
    "        return m\n",
    "    \n",
    "    def encode(self, inputs, encoder_state_input, embeddings, \n",
    "               dropout_keep_prob, is_training):\n",
    "        images, seq_len, questions, question_masks = inputs\n",
    "        self.batch_size = tf.shape(images)[0]\n",
    "        self.is_training = is_training\n",
    "        self.memory = []     \n",
    "        \n",
    "        means = tf.reshape(tf.constant(self.config.VGG_MEAN), [1, 1, 1, 3])\n",
    "        images = images - means \n",
    "    \n",
    "        vgg = tf.contrib.slim.nets.vgg\n",
    "        with slim.arg_scope(vgg.vgg_arg_scope(weight_decay=self.config.weight_decay)):\n",
    "            logits, end_points = vgg.vgg_16(images, num_classes=1, \n",
    "                                            is_training=False, dropout_keep_prob=1.0)\n",
    "            self.vgg_out = end_points['qa/vgg_16/conv5/conv5_3']\n",
    "            self.vgg_include = [end_points[e] for e in end_points if e not in self.config.vgg_exclude_names]\n",
    "            self.vgg_exclude = [end_points[e] for e in end_points if e not in self.config.vgg_exclude_names]\n",
    "            for e in self.vgg_include:\n",
    "                print(e)\n",
    "            print(\"vgg_out\", self.vgg_out)\n",
    "        \n",
    "        with tf.variable_scope('vqa_additional_encode') as vqa_add_scope:\n",
    "            # Encode question with GRU\n",
    "            questions_input = tf.nn.embedding_lookup(embeddings, questions)\n",
    "            with tf.variable_scope('q_encoder') as scope:\n",
    "                gru_cell = tf.contrib.rnn.GRUCell(self.config.rnn_hidden_size)\n",
    "                outputs, state = tf.nn.dynamic_rnn(cell=gru_cell,\n",
    "                                                   inputs=questions_input,\n",
    "                                                   sequence_length=question_masks,\n",
    "                                                   dtype=tf.float32)\n",
    "            # Question representation\n",
    "            self.q = state\n",
    "            self.memory.append(state) # m_0 = q\n",
    "            print(\"q\", self.q)\n",
    "            \n",
    "            self.image_seq = self.get_image_seq(self.vgg_out)\n",
    "                       \n",
    "            # Forward direction cell\n",
    "            gru_fw_cell = tf.contrib.rnn.GRUCell(self.config.rnn_hidden_size)\n",
    "            gru_bw_cell = tf.contrib.rnn.GRUCell(self.config.rnn_hidden_size)\n",
    "            outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell,\n",
    "                                                                     cell_bw=gru_bw_cell,\n",
    "                                                                     inputs=self.image_seq,\n",
    "                                                                     sequence_length=seq_len,\n",
    "                                                                     dtype=tf.float32)\n",
    "            self.F_bidir = outputs[0] + outputs[1]            \n",
    "            print(\"F_bidir\", self.F_bidir)\n",
    "            \n",
    "            with tf.variable_scope('mem1') as scope:\n",
    "                g_1 = self.get_attenton_g(f=self.F_bidir, q=self.q, m_prev=self.memory[0])\n",
    "                attn_cell1 =  AttnGRUCell(num_units=self.config.rnn_hidden_size,\n",
    "                                         attention=g_1, batch_normalize=self.config.batch_normalize, \n",
    "                                         is_training=self.is_training)\n",
    "                _, c_1 = tf.nn.dynamic_rnn(cell=attn_cell1,\n",
    "                                           inputs=self.F_bidir,\n",
    "                                           sequence_length=seq_len,\n",
    "                                           dtype=tf.float32)\n",
    "            \n",
    "#             c_1 = tf.get_variable(name=\"c_1\", shape=(1,self.config.rnn_hidden_size), \n",
    "#                                   dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "#             attn_cell1 = AttnGRU(num_units=self.config.rnn_hidden_size)\n",
    "#             with tf.variable_scope('Context1') as scope:\n",
    "#                 for i in range(self.config.images_input_sequece_len):\n",
    "#                     if i > 0: tf.get_variable_scope().reuse_variables()\n",
    "#                     c_1 = attn_cell1(inputs=self.F_bidir[:,i,:], \n",
    "#                                     state=c_1, attention=g_1[:,i])\n",
    "                    #print(\"c_1\", i, c_1)             \n",
    "            \n",
    "#             c_1 = tf.reduce_sum(self.F_bidir * tf.expand_dims(g_1, axis=2), axis=1)\n",
    "                print(\"c_1\", c_1)\n",
    "                m_1 = self.get_next_memory_state(m_prev=self.memory[0], c=c_1, q=self.q)\n",
    "                self.memory.append(m_1)\n",
    "            \n",
    "#             c_2 = tf.get_variable(name=\"c_2\", shape=(1,self.config.rnn_hidden_size), \n",
    "#                                   dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "                scope.reuse_variables()\n",
    "            \n",
    "            with tf.variable_scope('mem2') as scope:\n",
    "                g_2 = self.get_attenton_g(f=self.F_bidir, q=self.q, m_prev=self.memory[1])             \n",
    "\n",
    "                attn_cell2 = AttnGRUCell(num_units=self.config.rnn_hidden_size,\n",
    "                                         attention=g_2, batch_normalize=self.config.batch_normalize, \n",
    "                                         is_training=self.is_training)\n",
    "                _, c_2 = tf.nn.dynamic_rnn(cell=attn_cell2,\n",
    "                                           inputs=self.F_bidir,\n",
    "                                           sequence_length=seq_len,\n",
    "                                           dtype=tf.float32)\n",
    "    #             with tf.variable_scope('Context2') as scope:\n",
    "    #                 for i in range(self.config.images_input_sequece_len):\n",
    "    #                     if i > 0: tf.get_variable_scope().reuse_variables()\n",
    "    #                     c_2 = attn_cell2(inputs=self.F_bidir[:,i,:], \n",
    "    #                                     state=c_2, attention=g_1[:,i])\n",
    "\n",
    "                #c_2 = tf.reduce_sum(self.F_bidir * tf.expand_dims(g_2, axis=2), axis=1)\n",
    "                print(\"c_2\", c_2)\n",
    "                m_2 = self.get_next_memory_state(m_prev=self.memory[1], c=c_2, q=self.q)\n",
    "                self.memory.append(m_2)\n",
    "            \n",
    "        return self.memory[-1]           \n",
    "            \n",
    "            \n",
    "            \n",
    "class BaselineDecoder(Encoder):\n",
    "    def decode(self, knowledge_rep, dropout_keep_prob, is_training):\n",
    "        with tf.variable_scope('vqa_additional_decode') as vqa_add_scope:\n",
    "            scores = tf.layers.dense(inputs=knowledge_rep, units=self.config.num_classes, \n",
    "                                   activation=tf.nn.relu,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        return scores\n",
    "    \n",
    "    def plot(self, attn):\n",
    "#         print(attn)\n",
    "        plt.imshow(attn)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "vqa_encoder = BaselineEncoder(config=Config)\n",
    "vqa_decoder = BaselineDecoder(config=Config)\n",
    "\n",
    "vqa_system = VQASystem(encoder=vqa_encoder, decoder=vqa_decoder, \n",
    "                       pretrained_embeddings=np_embeddings, config=Config)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_saved_model = True\n",
    "with tf.Session() as sess:\n",
    "    util.initialize_model(sess, vqa_system, Config.model_dir, train_saved_model,config=Config)    \n",
    "    vqa_system.train(sess, dataset, best_score=0.0)\n",
    "#     vqa_system.evaluate_data(session=sess, sample_size=10000, \n",
    "#                              dataset=dataset.test, qid_to_anstype=qid_to_anstype, datatype=\"test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
