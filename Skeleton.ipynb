{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules needed\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "import util\n",
    "from vqa_model import Encoder, Decoder, VQASystem\n",
    "from squeezenet import SqueezeNet\n",
    "from vgg16 import VGG16\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Questions_train (1000, 25)\n",
      "Questions_mask_train (1000,)\n",
      "Image_ids_train (1000,)\n",
      "All_answers_train (1000, 10)\n",
      "Answers_train (1000,)\n",
      "************************************************************\n",
      "Questions_val (1000, 25)\n",
      "Questions_mask_val (1000,)\n",
      "Image_ids_val (1000,)\n",
      "All_answers_val (1000, 10)\n",
      "Answers_val (1000,)\n",
      "************************************************************\n",
      "Questions_test (1000, 25)\n",
      "Questions_mask_test (1000,)\n",
      "Image_ids_test (1000,)\n",
      "All_answers_test (1000, 10)\n",
      "Answers_test (1000,)\n",
      "************************************************************\n",
      "np_embeddings (47382, 100)\n",
      "************************************************************\n",
      "There are 10051 possible answers (including <unk>)\n",
      "This should be less than or equal to above 9862\n",
      "This should be less than or equal to above 8832\n",
      "This should be less than or equal to above 9640\n",
      "This should be less than or equal to above 9938\n",
      "This should be less than or equal to above 9862\n",
      "This should be less than or equal to above 9862\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Load data (excluding images)\n",
    "\n",
    "# Restrict the number of possible answers using this\n",
    "# Decreasing this will increase the number of classes \n",
    "train_min_count = 20\n",
    "val_cutoff = 107183\n",
    "# Load data\n",
    "dataset = util.load_data_all(train_min_count, val_cutoff=107183, limit=1000)\n",
    "np_embeddings = np.load(\"data/glove.trimmed.100.npz\")[\"glove\"]\n",
    "answer_to_id, id_to_answer = util.load_answer_map(min_count=train_min_count)\n",
    "\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_train\", dataset.train.questions.shape)\n",
    "print(\"Questions_mask_train\", dataset.train.mask.shape)\n",
    "print(\"Image_ids_train\", dataset.train.image_ids.shape)\n",
    "print(\"All_answers_train\", dataset.train.all_answers.shape)\n",
    "print(\"Answers_train\", dataset.train.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_val\", dataset.val.questions.shape)\n",
    "print(\"Questions_mask_val\", dataset.val.mask.shape)\n",
    "print(\"Image_ids_val\", dataset.val.image_ids.shape)\n",
    "print(\"All_answers_val\", dataset.val.all_answers.shape)\n",
    "print(\"Answers_val\", dataset.val.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_test\", dataset.test.questions.shape)\n",
    "print(\"Questions_mask_test\", dataset.test.mask.shape)\n",
    "print(\"Image_ids_test\", dataset.test.image_ids.shape)\n",
    "print(\"All_answers_test\", dataset.test.all_answers.shape)\n",
    "print(\"Answers_test\", dataset.test.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"np_embeddings\", np_embeddings.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"There are\", len(answer_to_id), \"possible answers (including <unk>)\")\n",
    "print(\"This should be less than or equal to above\", np.max(dataset.train.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.val.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.test.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.train.all_answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.train.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.train.answers) + 1) \n",
    "print(\"*\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd6e9b1bbe86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"Holds model hyperparams and data information.\n\u001b[1;32m      3\u001b[0m     \"\"\"\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cd6e9b1bbe86>\u001b[0m in \u001b[0;36mConfig\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmax_gradient_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mclip_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "    \"\"\"\n",
    "    epochs = 20\n",
    "    \n",
    "    learning_rate = 5e-4\n",
    "    optimizer = tf.train.AdamOptimizer\n",
    "    max_gradient_norm = 10.0\n",
    "    clip_gradients = False\n",
    "    dropout = 0.5\n",
    "    batch_size = 32\n",
    "    \n",
    "    max_question_length = 25    \n",
    "    num_answers_per_question = 10\n",
    "    num_classes = len(answer_to_id)\n",
    "    image_size = [224, 224, 3]\n",
    "    \n",
    "    images_input_sequece_len = 14*14\n",
    "    \n",
    "    rnn_hidden_size = 100 # RNN\n",
    "    fc_state_size = 100 # Fully connected\n",
    "    embedding_size = 100\n",
    "    \n",
    "    num_evaluate = 100\n",
    "    \n",
    "    print_every = 100 \n",
    "    \n",
    "    model_dir = \"skeleton_model\"\n",
    "    squeeze_net_dir = \"sq_net_model/squeezenet.ckpt\"\n",
    "    vgg16_weight_file = \"vgg_net_dir/vgg16_weights.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaselineEncoder(Encoder):\n",
    "    def compute_attention_g(self, level):\n",
    "        with tf.variable_scope('compute_attention_g'):\n",
    "            q = tf.expand_dims(self.q, axis=1)\n",
    "            m_before = tf.expand_dims(self.memory[level-1], axis=1)\n",
    "            z = tf.concat((self.F_bidir * q, self.F_bidir * m_before, \n",
    "                           tf.abs(self.F_bidir - q), tf.abs(self.F_bidir - m_before)), \n",
    "                          axis=2)\n",
    "            out = tf.Print(z, [tf.shape(z)], 'z = ', summarize=20, first_n=7)\n",
    "            print(\"z\", level, z)\n",
    "            \n",
    "            W_att_1 = tf.get_variable(name=\"W_att_1\", \n",
    "                                      shape=(4*self.config.rnn_hidden_size, \n",
    "                                             self.config.rnn_hidden_size), \n",
    "                                      dtype=tf.float32, \n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W_att_2 = tf.get_variable(name=\"W_att_2\", \n",
    "                                             shape=(self.config.rnn_hidden_size, 1), \n",
    "                                             dtype=tf.float32, \n",
    "                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_att_1 = tf.get_variable(name=\"b_att_1\", \n",
    "                                            shape=(self.config.rnn_hidden_size,), \n",
    "                                            dtype=tf.float32, \n",
    "                                            initializer=tf.constant_initializer(0.0))\n",
    "            b_att_2 = tf.get_variable(name=\"b_att_2\", \n",
    "                                            shape=(), \n",
    "                                            dtype=tf.float32, \n",
    "                                            initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "            z_reshaped = tf.reshape(z, [-1, 4*self.config.rnn_hidden_size])\n",
    "            print(\"z_reshaped\", level, z_reshaped)\n",
    "            Z_temp = tf.tanh(tf.matmul(z_reshaped, W_att_1) + b_att_1)\n",
    "            print(\"Z_temp\", level, Z_temp)\n",
    "            \n",
    "            Z_reshaped = tf.matmul(Z_temp, W_att_2) + b_att_2\n",
    "            print(\"Z_reshaped\", level, Z_reshaped)\n",
    "            \n",
    "            Z = tf.reshape(Z_reshaped, [self.batch_size, -1])\n",
    "            print(\"Z\", level, Z)\n",
    "            \n",
    "            exp_Z = tf.exp(Z)\n",
    "            g = exp_Z / (1e-6 + tf.reduce_sum(exp_Z, reduction_indices=1, keep_dims=True))\n",
    "            print(\"g\", g)\n",
    "                        \n",
    "            return g\n",
    "        \n",
    "    def get_next_memory_state(self, context, level):\n",
    "        with tf.variable_scope('compute_next_m'):\n",
    "            combo = tf.concat((self.memory[level-1], context, self.q), axis=1)\n",
    "            print(\"combo\", level, combo)\n",
    "            \n",
    "            W_nm_1 = tf.get_variable(name=\"W_nm_1\", \n",
    "                                      shape=(3*self.config.rnn_hidden_size, \n",
    "                                             self.config.rnn_hidden_size), \n",
    "                                      dtype=tf.float32, \n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_nm_1 = tf.get_variable(name=\"b_nm_1\", \n",
    "                                            shape=(self.config.rnn_hidden_size,), \n",
    "                                            dtype=tf.float32, \n",
    "                                            initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "            m = tf.nn.relu(tf.matmul(combo, W_nm_1) + b_nm_1) \n",
    "            print(\"m\", level, m)\n",
    "        return m\n",
    "            \n",
    "    \n",
    "    def encode(self, inputs, encoder_state_input, embeddings, dropout):\n",
    "        self.memory = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        images, seq_len, questions, question_masks = inputs\n",
    "        \n",
    "        self.batch_size = tf.shape(images)[0]\n",
    "        \n",
    "        self.vgg_net = VGG16(imgs=images, weights=Config.vgg16_weight_file)\n",
    "        #print(\"vgg_conv5_3\", self.vgg_net.conv5_3)\n",
    "        print(\"vgg_pool5\", self.vgg_net.pool5)\n",
    "        self.vgg_out = self.vgg_net.conv5_3\n",
    "        \n",
    "        \n",
    "        \n",
    "        #_, H, W, C = tf.shape(squeeze_net.features)\n",
    "        #print(tf.shape(squeeze_net.features))\n",
    "        with tf.variable_scope('vqa_additional'):\n",
    "            # Encode question with GRU\n",
    "            questions_input = tf.nn.embedding_lookup(embeddings, questions)\n",
    "            with tf.variable_scope('q_encoder') as scope:\n",
    "                gru_cell = tf.contrib.rnn.GRUCell(self.config.rnn_hidden_size)\n",
    "                outputs, state = tf.nn.dynamic_rnn(cell=gru_cell,\n",
    "                                                   inputs=questions_input,\n",
    "                                                   sequence_length=question_masks,\n",
    "                                                   #initial_state=image_feats,\n",
    "                                                   dtype=tf.float32)\n",
    "            # Question representation\n",
    "            self.q = state\n",
    "            self.memory.append(state) # m_0 = q\n",
    "            print(\"q\", self.q)\n",
    "                \n",
    "            F = tf.reshape(self.vgg_out, shape=(-1, 14*14, 512))\n",
    "            \n",
    "            print(\"F\", F)\n",
    "            \n",
    "            # Forward direction cell\n",
    "            gru_fw_cell = tf.contrib.rnn.GRUCell(self.config.rnn_hidden_size)\n",
    "            gru_bw_cell = tf.contrib.rnn.GRUCell(self.config.rnn_hidden_size)\n",
    "            outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell,\n",
    "                                                                     cell_bw=gru_bw_cell,\n",
    "                                                                     inputs=F,\n",
    "                                                                     sequence_length=seq_len,\n",
    "                                                                     dtype=tf.float32)\n",
    "            #print(\"outputs\", outputs)\n",
    "            #print(\"output_states\", output_states)\n",
    "            self.F_bidir = outputs[0] + outputs[1]            \n",
    "            print(\"F_bidir\", self.F_bidir)\n",
    "            \n",
    "            g_1 = self.compute_attention_g(level=1)       \n",
    "            c_1 = tf.reduce_sum(self.F_bidir * tf.expand_dims(g_1, axis=2), axis=1)\n",
    "            print(\"c_1\", c_1)\n",
    "            self.memory.append(self.get_next_memory_state(context=c_1, level=1))\n",
    "            \n",
    "            \n",
    "#             h_flat = tf.contrib.layers.flatten(self.vgg_out)\n",
    "#             print(\"h_flat\", h_flat)\n",
    "#             image_feats = tf.layers.dense(inputs=h_flat,\n",
    "#                                           units=self.config.rnn_hidden_size,\n",
    "#                                           activation=tf.nn.relu)\n",
    "            \n",
    "#             print(\"image_features\", image_feats)\n",
    "\n",
    "            \n",
    "        return self.memory[-1]\n",
    "            \n",
    "            \n",
    "        #return state\n",
    "\n",
    "class BaselineDecoder(Encoder):\n",
    "    def decode(self, knowledge_rep, dropout):\n",
    "        scores = tf.layers.dense(inputs=knowledge_rep, units=self.config.num_classes, \n",
    "                               activation=tf.nn.relu,\n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Adding Placeholers!\n",
      "Done Adding Embedding!\n",
      "vgg_pool5 Tensor(\"qa/pool4_1:0\", shape=(?, 7, 7, 512), dtype=float32)\n",
      "q Tensor(\"qa/vqa_additional/q_encoder/rnn/while/Exit_2:0\", shape=(?, 100), dtype=float32)\n",
      "F Tensor(\"qa/vqa_additional/Reshape:0\", shape=(?, 196, 512), dtype=float32)\n",
      "F_bidir Tensor(\"qa/vqa_additional/add:0\", shape=(?, 196, 100), dtype=float32)\n",
      "z 1 Tensor(\"qa/vqa_additional/compute_attention_g/concat:0\", shape=(?, 196, 400), dtype=float32)\n",
      "z_reshaped 1 Tensor(\"qa/vqa_additional/compute_attention_g/Reshape:0\", shape=(?, 400), dtype=float32)\n",
      "Z_temp 1 Tensor(\"qa/vqa_additional/compute_attention_g/Tanh:0\", shape=(?, 100), dtype=float32)\n",
      "Z_reshaped 1 Tensor(\"qa/vqa_additional/compute_attention_g/add_1:0\", shape=(?, 1), dtype=float32)\n",
      "Z 1 Tensor(\"qa/vqa_additional/compute_attention_g/Reshape_1:0\", shape=(?, ?), dtype=float32)\n",
      "g Tensor(\"qa/vqa_additional/compute_attention_g/truediv:0\", shape=(?, ?), dtype=float32)\n",
      "c_1 Tensor(\"qa/vqa_additional/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "combo 1 Tensor(\"qa/vqa_additional/compute_next_m/concat:0\", shape=(?, 300), dtype=float32)\n",
      "m 1 Tensor(\"qa/vqa_additional/compute_next_m/Relu:0\", shape=(?, 100), dtype=float32)\n",
      "encoding Tensor(\"qa/vqa_additional/compute_next_m/Relu:0\", shape=(?, 100), dtype=float32)\n",
      "scores Tensor(\"qa/dense/Relu:0\", shape=(?, 10051), dtype=float32)\n",
      "Done setting up system!\n",
      "Done setting up loss!\n",
      "Done adding training op!\n",
      "Graph setup done!!\n"
     ]
    }
   ],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "vqa_encoder = BaselineEncoder(config=Config)\n",
    "vqa_decoder = BaselineDecoder(config=Config)\n",
    "\n",
    "vqa_system = VQASystem(encoder=vqa_encoder, decoder=vqa_decoder, \n",
    "                       pretrained_embeddings=np_embeddings, config=Config)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "Number of params: 20966440 (retreival took 4.064116 secs)"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     saver = tf.train.Saver()\n",
    "#     saver.restore(sess, Config.squeeze_net_dir)\n",
    "    vqa_system.encoder.vgg_net.load_weights(weight_file=Config.vgg16_weight_file, sess=sess)\n",
    "    \n",
    "    vqa_system.train(sess, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# all_answers = tf.placeholder(tf.int64, [None, 5])\n",
    "# answers = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# def acc_count(t, val):\n",
    "#     t = tf.reshape(t, shape=(-1, 1))\n",
    "#     elements_equal_to_value = tf.equal(t, val)\n",
    "#     as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "#     count = tf.reduce_sum(as_ints, axis=1)\n",
    "#     accuracy = 1.0 * tf.minimum(count / 3, 1)\n",
    "#     return accuracy\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     v = sess.run([acc_count(answers, all_answers)], feed_dict={answers : np.array([1,2,3]),\n",
    "#                                                        all_answers : np.array([[1,2,1,1,1], [0,1,1,2,0], [3,3,3,1,0]])})\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(np.sum(dataset.train.answers == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(util.Progbar)\n",
    "# prog = util.Progbar(target=100)\n",
    "# for i in range(100):\n",
    "#     prog.update(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = tf.placeholder(tf.int32, [1,4,5])\n",
    "# b = tf.placeholder(tf.int32, [1,5])\n",
    "# func = tf.reduce_sum\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     #res = sess.run(func, {a : np.array([[1,2,3,4], [8,7,6,5], [9,10,11,12], [16,15,14,13]])})\n",
    "#     res = sess.run(func, {a : np.array([[[1,2,3,4,1], [8,7,6,5,1], [9,10,11,12,1], [16,15,14,13,1]]]),\n",
    "#                           b : np.array([[2,3,4,2,2]])})\n",
    "#     print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
