{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules needed\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "import util\n",
    "from vqa_model import Encoder, Decoder, VQASystem\n",
    "from squeezenet import SqueezeNet\n",
    "from vgg16 import VGG16\n",
    "\n",
    "from compact_bilinear_pooling import compact_bilinear_pooling_layer\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Questions_train (383270, 25)\n",
      "Questions_mask_train (383270,)\n",
      "Questions_ids_train (383270,)\n",
      "Image_ids_train (383270,)\n",
      "All_answers_train (383270, 10)\n",
      "Answers_train (383270,)\n",
      "************************************************************\n",
      "Questions_val (92423, 25)\n",
      "Questions_mask_val (92423,)\n",
      "Questions_ids_val (92423,)\n",
      "Image_ids_val (92423,)\n",
      "All_answers_val (92423, 10)\n",
      "Answers_val (92423,)\n",
      "************************************************************\n",
      "Questions_test (92162, 25)\n",
      "Questions_mask_test (92162,)\n",
      "Questions_ids_test (92162,)\n",
      "Image_ids_test (92162,)\n",
      "All_answers_test (92162, 10)\n",
      "Answers_test (92162,)\n",
      "************************************************************\n",
      "np_embeddings (47382, 100)\n",
      "************************************************************\n",
      "There are 3004 possible answers (including <unk>)\n",
      "This should be less than or equal to above 3004\n",
      "This should be less than or equal to above 3001\n",
      "This should be less than or equal to above 3004\n",
      "This should be less than or equal to above 3004\n",
      "This should be less than or equal to above 3004\n",
      "This should be less than or equal to above 3004\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Load data (excluding images)\n",
    "\n",
    "# Restrict the number of possible answers using this\n",
    "# Decreasing this will increase the number of classes \n",
    "train_min_count = 99\n",
    "val_cutoff = 107183\n",
    "# Load data\n",
    "dataset = util.load_data_all(train_min_count, val_cutoff=107183, limit=100000000)\n",
    "np_embeddings = np.load(\"data/glove.trimmed.100.npz\")[\"glove\"]\n",
    "answer_to_id, id_to_answer = util.load_answer_map(min_count=train_min_count)\n",
    "with open('data/qid_to_anstype.dat', 'rb') as fp:\n",
    "    qid_to_anstype = pickle.load(fp)\n",
    "\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_train\", dataset.train.questions.shape)\n",
    "print(\"Questions_mask_train\", dataset.train.mask.shape)\n",
    "print(\"Questions_ids_train\", dataset.train.question_ids.shape)\n",
    "print(\"Image_ids_train\", dataset.train.image_ids.shape)\n",
    "print(\"All_answers_train\", dataset.train.all_answers.shape)\n",
    "print(\"Answers_train\", dataset.train.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_val\", dataset.val.questions.shape)\n",
    "print(\"Questions_mask_val\", dataset.val.mask.shape)\n",
    "print(\"Questions_ids_val\", dataset.val.question_ids.shape)\n",
    "print(\"Image_ids_val\", dataset.val.image_ids.shape)\n",
    "print(\"All_answers_val\", dataset.val.all_answers.shape)\n",
    "print(\"Answers_val\", dataset.val.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"Questions_test\", dataset.test.questions.shape)\n",
    "print(\"Questions_mask_test\", dataset.test.mask.shape)\n",
    "print(\"Questions_ids_test\", dataset.test.question_ids.shape)\n",
    "print(\"Image_ids_test\", dataset.test.image_ids.shape)\n",
    "print(\"All_answers_test\", dataset.test.all_answers.shape)\n",
    "print(\"Answers_test\", dataset.test.answers.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"np_embeddings\", np_embeddings.shape)\n",
    "print(\"*\" * 60)\n",
    "print(\"There are\", len(answer_to_id), \"possible answers (including <unk>)\")\n",
    "print(\"This should be less than or equal to above\", np.max(dataset.train.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.val.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.test.answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.train.all_answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.val.all_answers) + 1) \n",
    "print(\"This should be less than or equal to above\", np.max(dataset.test.all_answers) + 1) \n",
    "print(\"*\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print((dataset.train.answers == 0).sum())\n",
    "# print((dataset.val.answers == 0).sum())\n",
    "# print((dataset.test.answers == 0).sum())\n",
    "# print((dataset.train.questions[dataset.train.answers == 0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "    \"\"\"\n",
    "    epochs = 5\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    lr_decay = 0.9\n",
    "    optimizer = tf.train.AdamOptimizer\n",
    "    max_gradient_norm = 10.0\n",
    "    clip_gradients = True\n",
    "    dropout_keep_prob = 1.0\n",
    "    l2_reg = 0.0\n",
    "    batch_size = 64\n",
    "    train_all = False\n",
    "    weight_decay = 5e-4\n",
    "    \n",
    "    train_limit = 100000000\n",
    "    \n",
    "    max_question_length = 25    \n",
    "    num_answers_per_question = 10\n",
    "    num_classes = len(answer_to_id)\n",
    "    image_size = [224, 224, 3]\n",
    "    cbpl_output_dim = 8000\n",
    "    att_conv1_dim = 512\n",
    "    \n",
    "    vgg_out_dim = [14, 14]\n",
    "    \n",
    "    images_input_sequece_len = vgg_out_dim[0] * vgg_out_dim[1]\n",
    "    \n",
    "    rnn_hidden_size = 512 # RNN\n",
    "    fc_state_size = 100 # Fully connected\n",
    "    embedding_size = 100\n",
    "    \n",
    "    num_evaluate = 5000\n",
    "    \n",
    "    eval_every = 100\n",
    "    print_every = 100 \n",
    "    \n",
    "    VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "    \n",
    "    model_dir = \"new_bilinear_model\"\n",
    "    squeeze_net_dir = \"sq_net_model/squeezenet.ckpt\"\n",
    "    vgg16_weight_file = \"vgg_net_dir/vgg16_weights.npz\"\n",
    "    \n",
    "    vgg16_weight_file = \"vgg_net_dir/vgg_16.ckpt\"\n",
    "    vgg_out_dim = [14, 14]\n",
    "    \n",
    "    vgg_exclude_names = ['qa/vgg_16/pool5', 'qa/vgg_16/fc6', 'qa/vgg_16/fc7', 'qa/vgg_16/fc8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaselineEncoder(Encoder):\n",
    "    \n",
    "            \n",
    "    \n",
    "    def encode(self, inputs, encoder_state_input, embeddings, \n",
    "               dropout_keep_prob, is_training):\n",
    "        images, _, questions, question_masks = inputs\n",
    "        self.batch_size = tf.shape(images)[0]\n",
    "        \n",
    "#         self.vgg_net = VGG16(imgs=images, weights=Config.vgg16_weight_file)\n",
    "#         self.vgg_out = self.vgg_net.conv5_3\n",
    "#         self.vgg_out = tf.nn.l2_normalize(self.vgg_out, dim=3)\n",
    "#         print(\"vgg_out\", self.vgg_out)\n",
    "        \n",
    "        means = tf.reshape(tf.constant(self.config.VGG_MEAN), [1, 1, 1, 3])\n",
    "        images = images - means \n",
    "    \n",
    "        vgg = tf.contrib.slim.nets.vgg\n",
    "        with slim.arg_scope(vgg.vgg_arg_scope(weight_decay=self.config.weight_decay)):\n",
    "            logits, end_points = vgg.vgg_16(images, num_classes=1, \n",
    "                                            is_training=False, dropout_keep_prob=1.0)\n",
    "            self.vgg_out = end_points['qa/vgg_16/conv5/conv5_3']\n",
    "            self.vgg_include = [end_points[e] for e in end_points if e not in self.config.vgg_exclude_names]\n",
    "            self.vgg_exclude = [end_points[e] for e in end_points if e not in self.config.vgg_exclude_names]\n",
    "            for e in self.vgg_include:\n",
    "                print(e)\n",
    "            self.vgg_out = tf.nn.l2_normalize(self.vgg_out, dim=3)\n",
    "            print(\"vgg_out\", self.vgg_out)\n",
    "        \n",
    "        with tf.variable_scope('vqa_additional'):\n",
    "            # Encode question with GRU\n",
    "            questions_input = tf.nn.embedding_lookup(embeddings, questions)\n",
    "            questions_input = tf.tanh(questions_input)\n",
    "            with tf.variable_scope('q_encoder') as scope:\n",
    "                gru_cell = tf.contrib.rnn.GRUCell(self.config.rnn_hidden_size)\n",
    "#                 gru_cell = tf.contrib.rnn.DropoutWrapper(gru_cell,\n",
    "#                                                          input_keep_prob=dropout_keep_prob,\n",
    "#                                                          output_keep_prob=dropout_keep_prob)\n",
    "                outputs, state = tf.nn.dynamic_rnn(cell=gru_cell,\n",
    "                                                   inputs=questions_input,\n",
    "                                                   sequence_length=question_masks,\n",
    "                                                   dtype=tf.float32)\n",
    "            # Question representation\n",
    "            self.q_enc = state\n",
    "#             self.q_enc = tf.tanh(self.q_enc)\n",
    "            self.q_enc = tf.nn.dropout(self.q_enc, keep_prob=dropout_keep_prob)\n",
    "            print(\"q_enc\", self.q_enc)\n",
    "            \n",
    "            tile_temp = tf.reshape(self.q_enc, shape=(-1, 1, 1, self.config.rnn_hidden_size))\n",
    "            self.q_tile = tf.tile(tile_temp, [1] + self.config.vgg_out_dim + [1])\n",
    "            print(\"q_tile\", self.q_tile)\n",
    "            \n",
    "            self.q_im_attn = compact_bilinear_pooling_layer(bottom1=self.vgg_out, bottom2=self.q_tile, \n",
    "                                                            output_dim=self.config.cbpl_output_dim,\n",
    "                                                            sum_pool=False)\n",
    "            \n",
    "            self.q_im_attn = tf.reshape(self.q_im_attn, shape=[-1]+ self.config.vgg_out_dim + [self.config.cbpl_output_dim])\n",
    "            self.q_im_attn += 1e-12\n",
    "            self.q_im_attn = tf.sign(self.q_im_attn) * tf.sqrt(tf.abs(self.q_im_attn))\n",
    "            self.q_im_attn = tf.nn.l2_normalize(self.q_im_attn, dim=3)\n",
    "#             self.q_im_attn = tf.layers.batch_normalization(self.q_im_attn)\n",
    "            print(\"q_im_attn\", self.q_im_attn)\n",
    "            \n",
    "#             self.att_Wconv1 = tf.get_variable(name=\"att_Wconv1_weight\", \n",
    "#                                               shape=[3, 3, self.config.cbpl_output_dim, \n",
    "#                                                      self.config.att_conv1_dim], \n",
    "#                                               dtype=tf.float32, \n",
    "#                                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.att_Wconv1 = tf.Variable(tf.truncated_normal([3, 3, self.config.cbpl_output_dim, \n",
    "                                                               self.config.att_conv1_dim], \n",
    "                                                              dtype=tf.float32,\n",
    "                                                              stddev=1e-1), \n",
    "                                          name='att_Wconv1_weight')\n",
    "            self.att_bconv1 = tf.Variable(tf.constant(0.0, shape=[self.config.att_conv1_dim], dtype=tf.float32),\n",
    "                                 trainable=True, name='att_bconv1')\n",
    "            self.attn_conv1 = tf.nn.conv2d(self.q_im_attn,self.att_Wconv1,\n",
    "                                           strides=[1,1,1,1], padding='SAME') + self.att_bconv1\n",
    "            self.attn_conv1 = tf.nn.relu(self.attn_conv1)\n",
    "#             self.attn_conv1 = tf.layers.batch_normalization(self.attn_conv1)\n",
    "            print(\"attn_conv1\", self.attn_conv1)\n",
    "            \n",
    "#             self.att_Wconv2 = tf.get_variable(name=\"att_Wconv2_weight\", \n",
    "#                                               shape=[3, 3, self.config.att_conv1_dim, 1], \n",
    "#                                               dtype=tf.float32, \n",
    "#                                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.att_Wconv2 =  tf.Variable(tf.truncated_normal([3, 3, self.config.att_conv1_dim, 1], \n",
    "                                                              dtype=tf.float32,\n",
    "                                                              stddev=1e-1), \n",
    "                                          name='att_Wconv2_weight')\n",
    "            self.att_bconv2 = tf.Variable(tf.constant(0.0, shape=[1], dtype=tf.float32),\n",
    "                                 trainable=True, name='att_bconv2')\n",
    "            self.attn_conv2 = tf.nn.conv2d(self.attn_conv1, self.att_Wconv2,\n",
    "                                           strides=[1,1,1,1], padding='SAME') + self.att_bconv2\n",
    "            print(\"attn_conv2\", self.attn_conv2)\n",
    "            \n",
    "            self.attn_flat = tf.reshape(self.attn_conv2, shape=[-1, self.config.images_input_sequece_len])\n",
    "            self.alpha = tf.nn.softmax(self.attn_flat)\n",
    "            self.alpha = tf.reshape(self.alpha, shape=[-1] + self.config.vgg_out_dim + [1])\n",
    "            print(\"alpha\", self.alpha)\n",
    "            \n",
    "            weighted = self.alpha * self.vgg_out\n",
    "            print(\"weighted\",weighted)\n",
    "            self.attended_image = tf.reduce_sum(weighted, axis=(1,2))\n",
    "            self.attended_image = tf.nn.dropout(self.attended_image, keep_prob=dropout_keep_prob)\n",
    "            print(\"attended_image\", self.attended_image)\n",
    "            \n",
    "            a=tf.reshape(self.attended_image, shape=(-1, 1, 1, self.config.rnn_hidden_size), name=\"HERE1\")\n",
    "            b=tf.reshape(self.q_enc, shape=(-1, 1, 1, self.config.rnn_hidden_size), name=\"HERE2\")\n",
    "            print(\"a\", a)\n",
    "            print(\"b\", b)\n",
    "            \n",
    "            self.attd_im_q = compact_bilinear_pooling_layer(bottom1=a, \n",
    "                                                            bottom2=b,  \n",
    "                                                             output_dim=self.config.cbpl_output_dim,\n",
    "                                                             sum_pool=False)\n",
    "            self.attd_im_q = tf.reshape(self.attd_im_q, shape=[-1, self.config.cbpl_output_dim])\n",
    "            self.attd_im_q += 1e-12\n",
    "            self.attd_im_q = tf.sign(self.attd_im_q) * tf.sqrt(tf.abs(self.attd_im_q))\n",
    "            self.attd_im_q = tf.nn.l2_normalize(self.attd_im_q, dim=1)\n",
    "#             self.attd_im_q = tf.layers.batch_normalization(self.attd_im_q)\n",
    "            \n",
    "            \n",
    "            print(\"attd_im_q\", self.attd_im_q)\n",
    "            \n",
    "        return self.attd_im_q\n",
    "        #return state\n",
    "\n",
    "class BaselineDecoder(Encoder):\n",
    "    def decode(self, knowledge_rep, dropout_keep_prob, is_training):\n",
    "        with tf.variable_scope('vqa_additional_decode') as vqa_add_scope:\n",
    "            scores = tf.layers.dense(inputs=knowledge_rep, units=self.config.num_classes, \n",
    "                                   activation=tf.nn.relu,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        return scores\n",
    "    \n",
    "    def plot(self, attn):\n",
    "#         print(attn)\n",
    "#         plt.imshow(attn)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Adding Placeholers!\n",
      "Done Adding Embedding!\n",
      "Tensor(\"qa/vgg_16/conv1/conv1_1/Relu:0\", shape=(?, 224, 224, 64), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv1/conv1_2/Relu:0\", shape=(?, 224, 224, 64), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/pool1/MaxPool:0\", shape=(?, 112, 112, 64), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv2/conv2_1/Relu:0\", shape=(?, 112, 112, 128), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv2/conv2_2/Relu:0\", shape=(?, 112, 112, 128), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/pool2/MaxPool:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv3/conv3_1/Relu:0\", shape=(?, 56, 56, 256), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv3/conv3_2/Relu:0\", shape=(?, 56, 56, 256), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv3/conv3_3/Relu:0\", shape=(?, 56, 56, 256), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/pool3/MaxPool:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv4/conv4_1/Relu:0\", shape=(?, 28, 28, 512), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv4/conv4_2/Relu:0\", shape=(?, 28, 28, 512), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv4/conv4_3/Relu:0\", shape=(?, 28, 28, 512), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/pool4/MaxPool:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv5/conv5_1/Relu:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv5/conv5_2/Relu:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"qa/vgg_16/conv5/conv5_3/Relu:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "vgg_out Tensor(\"qa/l2_normalize:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "q_enc Tensor(\"qa/vqa_additional/dropout/mul:0\", shape=(?, 512), dtype=float32)\n",
      "q_tile Tensor(\"qa/vqa_additional/Tile:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "q_im_attn Tensor(\"qa/vqa_additional/l2_normalize:0\", shape=(?, 14, 14, 8000), dtype=float32)\n",
      "attn_conv1 Tensor(\"qa/vqa_additional/Relu:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "attn_conv2 Tensor(\"qa/vqa_additional/add_2:0\", shape=(?, 14, 14, 1), dtype=float32)\n",
      "alpha Tensor(\"qa/vqa_additional/Reshape_6:0\", shape=(?, 14, 14, 1), dtype=float32)\n",
      "weighted Tensor(\"qa/vqa_additional/mul_1:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "attended_image Tensor(\"qa/vqa_additional/dropout_1/mul:0\", shape=(?, 512), dtype=float32)\n",
      "a Tensor(\"qa/vqa_additional/HERE1:0\", shape=(?, 1, 1, 512), dtype=float32)\n",
      "b Tensor(\"qa/vqa_additional/HERE2:0\", shape=(?, 1, 1, 512), dtype=float32)\n",
      "attd_im_q Tensor(\"qa/vqa_additional/l2_normalize_1:0\", shape=(?, 8000), dtype=float32)\n",
      "encoding Tensor(\"qa/vqa_additional/l2_normalize_1:0\", shape=(?, 8000), dtype=float32)\n",
      "scores Tensor(\"qa/vqa_additional_decode/dense/Relu:0\", shape=(?, 3004), dtype=float32)\n",
      "Done setting up system!\n",
      "Done setting up loss!\n",
      "11\n",
      "11\n",
      "Done adding training op!\n",
      "pred_answer Tensor(\"qa/Cast:0\", shape=(?,), dtype=int32)\n",
      "correct_pred Tensor(\"qa/Equal:0\", shape=(?,), dtype=bool)\n",
      "Graph setup done!!\n"
     ]
    }
   ],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "vqa_encoder = BaselineEncoder(config=Config)\n",
    "vqa_decoder = BaselineDecoder(config=Config)\n",
    "\n",
    "vqa_system = VQASystem(encoder=vqa_encoder, decoder=vqa_decoder, \n",
    "                       pretrained_embeddings=np_embeddings, config=Config)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with fresh parameters.\n",
      "Num params: 200848534\n",
      "Number of params: 200848534 (retreival took 2.208189 secs)"
     ]
    }
   ],
   "source": [
    "train_saved_model = False\n",
    "with tf.Session() as sess:\n",
    "    util.initialize_model(sess, vqa_system, Config.model_dir, train_saved_model,config=Config)    \n",
    "    vqa_system.train(sess, dataset)\n",
    "#     for t in tf.trainable_variables():\n",
    "#         print(t)\n",
    "#     print(\"*\" * 20)   \n",
    "#     for t in tf.trainable_variables():\n",
    "#         if \"vqa_additional\" in t.name:\n",
    "#             print(t)\n",
    "#     vqa_system.evaluate_data(session=sess, sample_size=10000, \n",
    "#                              dataset=dataset.test, qid_to_anstype=qid_to_anstype, datatype=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# all_answers = tf.placeholder(tf.int64, [None, 5])\n",
    "# answers = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# def acc_count(t, val):\n",
    "#     t = tf.reshape(t, shape=(-1, 1))\n",
    "#     elements_equal_to_value = tf.equal(t, val)\n",
    "#     as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "#     count = tf.reduce_sum(as_ints, axis=1)\n",
    "#     accuracy = 1.0 * tf.minimum(count / 3, 1)\n",
    "#     return accuracy\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     v = sess.run([acc_count(answers, all_answers)], feed_dict={answers : np.array([1,2,3]),\n",
    "#                                                        all_answers : np.array([[1,2,1,1,1], [0,1,1,2,0], [3,3,3,1,0]])})\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(np.sum(dataset.train.answers == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(util.Progbar)\n",
    "# prog = util.Progbar(target=100)\n",
    "# for i in range(100):\n",
    "#     prog.update(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, [1,4,5])\n",
    "b = tf.placeholder(tf.int32, [1,5])\n",
    "func = tf.nn.l2_normalize(a, dim=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #res = sess.run(func, {a : np.array([[1,2,3,4], [8,7,6,5], [9,10,11,12], [16,15,14,13]])})\n",
    "    res = sess.run(func, {a : np.array([[[1,2,3,4,1], \n",
    "                                         [8,7,6,5,1], \n",
    "                                         [9,10,11,12,1], \n",
    "                                         [16,15,14,13,1]]]),\n",
    "                          b : np.array([[2,3,4,2,2]])})\n",
    "    print(res)\n",
    "# print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
